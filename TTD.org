* SEARCH SANITY CHECK 
- why the following two returns different results? 
"a bus collision with a truck in uganda has resulted in at least 30 fatalities and has left a further 21 injured"
"30 die in a bus collision in uganda" 
- write a simple script and test: "bus" "bus collision" "bus collision in uganda"  
- (I am expecting all OR relation. is it something not?) 
** DONE result return script 
** DONE test those sequence 
- it was because of "and". :-( 
** DONE check all "special words" for (P)lucene query. 
- A more complex queries may contain nested queries with 
 'and', 'or', 'not' or 'phrase' relations. (PLUCENE::SEARCH::QUERY)
** TODO imporve plucene_query() by removing those terms from the given query

* PERFORMANCE WORK 
- large files in a dir makes (10k>) file locating very, very slow. 
- GOAL: to make calling "ngram" perl doc as fast as "non-indexed" callings.  

** GZSet to use Month as dir 
- make gzset unzipper to use "months" too. This will reduce the number of files in dir. 
*** DONE writing 
*** DONE testing 

** Sort index hit result 
*** DONE writing 
- this will (maybe) make it faster to process indexed ones. (test on gillespie afp2010) 
*** DONE testing 
- Only a few dozen seconds. It affects, but not enough. 

** Index loading only once 
- indexer, as a global. It will be loaded only if it is null. 
*** TODO writing 
*** TODO testing 

** Getting list of all model files, only once 
- path recorder, as a global (same as index). It will be loaded only once, if it is null 
*** TODO writing 
*** TODO testing 

* (POSTPONED FOR THE MOMENT) Testing Work 
*** TODO Data preparation
**** TODO All AFP 
- Westy and Gillespie/proj
**** TODO 2009, 2010 
- Westy 
**** TODO 2010 
- Gillespie/home 

*** TODO Test with some sentences from RTE3, for next step. 
*** TODO Make a experiment sketch code that will read and work on RTE3 data. 
*** TODO Collect first full list of "gain", for RTE3 with default Lambda. 
- gain, P(t), P(h), (as it is, and collection backoff), P(h|t), t-length,
  h-length
- copy and backup evidence file somewhere, 


* MEMO 
** Some numbers 
- ALL, 100k (all, but no stop-word), 1k, 100, 10, etc 
*** ALL 
P(h) is (logprob): -8.0458 
P(h|t) is (logprob):  -7.0918 
P(h|t) / P(h) is (nonlog): 8.99497581530035
the code took:166 wallclock secs (14.09 usr 120.09 sys + 188.45 cusr 163.45 csys = 486.08 CPU)
*** 100k (all, but excluding stop word hits) 
P(h) is (logprob): -8.0458 
P(h|t) is (logprob):  -7.0889 
P(h|t) / P(h) is (nonlog): 9.05524072268361
the code took:67 wallclock secs ( 8.02 usr 35.87 sys + 70.79 cusr 51.31 csys = 165.99 CPU)
*** 1k 
P(h) is (logprob): -8.1412 
P(h|t) is (logprob):  -7.1033 
P(h|t) / P(h) is (nonlog): 10.911890519552
the code took:25 wallclock secs ( 4.34 usr  5.96 sys + 21.98 cusr  8.19 csys = 40.47 CPU)
*** 100 
P(h) is (logprob): -8.5588 
P(h|t) is (logprob):  -7.7297 
P(h|t) / P(h) is (nonlog): 6.74683361398827
the code took:19 wallclock secs ( 3.73 usr  1.45 sys + 13.94 cusr  1.04 csys = 20.16 CPU)
*** 10
(h) is (logprob): -8.9819 
P(h|t) is (logprob):  -9.0917 
P(h|t) / P(h) is (nonlog): 0.776604673960837
the code took:18 wallclock secs ( 3.63 usr  0.84 sys + 13.14 cusr  0.33 csys = 17.94 CPU)




** Speed test 
- see speedtest.txt 


* Consider The following Improvements 
** TODO MAYBE? Normalized contribution (of each doc to P(h|t)) 


* Currently
- Working on Index Work Improvements & Testing Work 



* PRESENTPRESENTPRESENTPRESENTPRESENTPRESENTPRESENTPRESENT
* PASTPASTPASTPASTPASTPASTPASTPASTPASTPASTPASTPASTPASTPAST

* DEVEL History 
** (Stored) Future Improvements 
*** [#B] remove (or don't count) too short news articles
- 100 bytes? there are some weird (not normal) news files even in
  .story files 

** Previous Improvements 
*** [#A] Collection model generate from subdirs 
**** DONE For collection - catall to dump everything in subdirs.  

*** [#A] Per doc model generate with subdirs 
**** DONE For per-doc models - perstroy_runnner with subdir. 
- now it works on all files in the given dir and its direct sub dirs 
*** [#A] SubDir plays (needed before doing more than AFP one year) 
**** DONE P_t should traverse all subdirs. 
- P_t argument change (and all consequent callers) 
- P_t code change (to traverse and run) 
**** DONE Test of P_h_t_multithread with sketch 
- (with multiple subdirs) 

*** [#A] Index Work 
*** DONE add index optimizer at the end of indexing.pl 
- check doc Plucene::Index::Writer 
- call optimize before close the writer. 
- output indexed file number via $writer->doc_count; 

*** DONE run temp.pl query on 2009 data 
- run something (on temp already) 
- check & compare, make sure it really works. (It seems so. Seems so
  doesn't sound so strict, but I have no other reason to belive it
  won't work so :-). 

*** DONE recreate the work environment in home ... 

*** DONE query method. (Text in, ordered result out) 
- with test code. Yeah! 

*** DONE Code P_t with index. 
- ... and how? 
- ... spend some time ... 

*** DONE Implement "top N" approximation. check some approx vs non-approx.
- 

*** DONE Some more test on P_t, "and try approximate"?
- Is it Okay to use top_N? say, 10k? Spend some time. 
- Approximation will (artificially) lower P_t(hypo). 
- But it will also lower P_t(text) and everything (?) 
- What we do finally is comparing P(hypo) and P(hypo | text): if two
  things both got lowered. Is this Acceptable?  ... 
- Need more testing. 
- It drops "too much". (very easily get "min" value). Very large Big tail.  

*** DONE Implement P_t_h_index with N approximation. 
*** DONE Test P_t_h_index with test code. 
*** DONE Play with some more simple texts on the newst implementation. 

** Main line coding 
*** DONE Collection Model 
**** DONE (run) Get "target" news files (target corpus) all in one folder 
**** DONE (run) catall and generate collection LM model 
**** DONE [#C] (If subdir needed) TODO? (write script) recursively catall and generate collection model 
*** DONE Document Model 
**** DONE (write script) For each file, make each LM model
*** DONE Produce single sentence prob. (t) 
**** DONE (write matlab script) weighted-sum 
- input: weight (doc prob), sentence prob, of each document 
- output: weighted average. 
**** (write scripts) P(t) prob 
***** DONE (write debug3 reader) read_log_prob, read_prob
***** DONE (write octave caller) lambda sum (interpolate) 
***** DONE check code for get seq_prob to lambda sum 
***** DONE (srilm caller) write ngram runner
- model 
- options  
- sentence (input) 
***** DONE (write octave caller) weighted sum 
- (need): weighted-sum input format (simple matrix)?
- (already have): weighted-sum matlab code 
***** DONE (write octave caller wrapper) logprob mean 
- use weighted sum with same weights. :-) 
***** DONE calc P_coll 
- check collection model file 
- get P_coll (t) (with -debug 3)
***** DONE each P_doc(t) 
- get for each pure P_d(t) (with -debug 3), on all doc 
- calculate lamda*P_d + (1-lamda)*P_coll for each by call octave
***** DONE calc P_(t) by weighted sum 
- do the weighted-sum of the values, with uniform weight 

*** DONE Produce conditional prob. 
**** DONE (write scripts) P(h | t) prob 
**** DONE write script "evidence calculation code" 
Wow. Finally. 
**** DONE sanity check, more with sketch. 
- on AFP 2009 May

**** DONE [#A] Some possible "look-into" data saving. 
- Starting from P_t, P_h, P_h|t. 
- Output of result hash: 
- Debug 1 : output the hash into file, no sorting, file order 
- Debug 2 : sorting, higher value first. 


* EXPERIMENTS 
** Need to confirm/consider 
*** TODO very long sentence okay. (-200 or less logprob) 
- pick one or two "paragraph" level "Text". Test it. 

** MODEL preparation 
*** DONE [#A] See how ngram-count works on large files 
**** DONE 1) afp 2010 (no problem) 
**** DONE 1-b) afp 2010 per doc (no problem) 
**** DONE 2) all afp. (Gillespie, no problem)
**** DONE 2-b) all afp, per doc (Gillespie, ONGOING) 
- Way too slow (no need to do, since 2010 takes 30+ min) 
**** TODO 3) all of the gigaword? 
- Maybe we need something between 2), 3). 

** Some additional ideas 
*** some rough ideas & observations 
- better baseline would be P(h|h), instead of P(h)? (topical
  relatedness gets some even before starting). 
- "gain" (P(h|t) / P(h)) seems to (generally) increases with the
  length of (t & h)  
  
** Notes 
*** Currently used/tested SRILM call parameters 
**** ngram-count 
- (CURRENT) "-text" and "-lm", and "-write-binary-lm", all other
  default 
**** ngram 
- (CURRENT) all default: no other than "-ppl" (input designation) and "-lm".  

*** Memo on efficiency
**** Testing on May 2009 AFP news (20k documents) 
- Running P_t sequentially currently takes about 3 min (2:48) on Moore.  
- Multi threads (6) on Gillespie, 58 seconds 


* RECORDS & POSTPONED
** Past Improvements 
*** DONE Binary language model 
*** add binary option as default option 
**** DONE collection model description (user's own calling) 
**** DONE perstory_runner.pl (per document model) 
- I believe that ngram automatically loads binary model, so no
  additional coding on model users.  

*** DONE [#A] bug splitta outputs the last "." concatted to the last Word.    
*** TODO? [#C] [??] feature catall.pl "do not print a file size less than X" 
*** TODO? [#C] [Very hard - Possible?] Matrix-ize weighted_sum Octave code. 
*** DONE [#A] [Efficiency] Lamda sum in Perl space. (No octave call) 
- For each news "story" we call twice; once ngram (can't reduce this),
  once octave. Maybe starting up octave each time is
  expansive. Consider this. 
*** DONE [#A] [Efficiency for response] Not using multiple threads/ngram processes

** Postponed improvements: "Good to have, but not critical"
*** TODO? [#C] [Efficiency for throughput] Unable to call two or more instances. 
- Currently, the file to be passed to ngram -ppl is a fixed name. 
- should be improved to temporary random name, or something like
  getName{sent}?
- *Not really important*, since the code does use multithread for P_t, and a
  single instance can utilize many nubmers of threads. 
*** TODO? [#C] If log-sum is only needed as "weighted sum" (use not-tool-small sum)
- we may not need to do the costy log-space-sums. 
- (by multiply weights to a certain degree, so within octave normal range). 
- (using reference_weightedsum, or a improved variation, etc). 
- *Not really important* Only calculated twice, or three times only
  per each P(h|t). Not really critical, compared to other
  efficiency issues. 
- Well, "not needing octave anymore" would be nice but. 

==== 
** Known problems
*** Discount related questions
- When processing document-models; 
- "Warning: count of count x is zero -- lowering maxcount" 
- "Warning: discount coeff n is out of range: 0" 
It seems that both related to sparseness. Not critical, but affecting
(e.g. less good smoothing?)  
** Side notes about tools 
*** SRILM 
**** Interpolate call parameters 
- "-bayes 0" mix-model is generally what I would expect from simple
  summation: simple (lambda * model 1 prob) + ((1-lamba) * model 2
  prob), for each word point. (Well if you ask me what -bayes non-zero
  means ... I don't) 
- so the mixture model call is something like: 
- ngram -lm doc.model -mix-lm collection.model -ppl test.txt -bayes 0 -debug 3 -lambda 0.1

**** Perplexity (per word), as calculated in SRILM 
- ppl = 10^(-logprob / (words - OOVs + sentences))
- ppl1 (without </s>) = 10^(-logprob / (words - OOVs)) 

**** Discount methods in SRILM defult 
- When no option is given, it does Good-Turing discount. (the warnings
  are from those, when counting count of counts, etc) 

**** Why different prob, for all OOV queries? 
- Q: They share all the same back-off interpolate model, why different? 
- A: /s 
- All OOV docs, at least has one </s>. Different /s prob per models. 
- We now have an option to exclude this </s>, from calculation. (DEFAULT ON, on lamba_sumX) 

*** Octave 
**** Octave "precision" of double is one digit less (than SRILM) 
- Seems like this causes the small amount of difference in the final
  result. (try octave> a = 0.00409898) 
- Octave uses H/W floats. ... hmm. no easy way around(?)
- Eh, no. Above examples is actually within HW float, but octave cuts 
  it. Prolly some precision cut mechanism in work. What's it? 
- "Symbolic toolbox". vpa(something)? Hmm. no need yet.  

** Theoretical crosspoints / decisions 


* THEORETICAL 
** DONE [#A] Word level model, or Sentence level model? 
- Basically, what I am trying to do is doing weighted sum of
  probabilities. There is two way of doing things. 
- Word Level weighted sum and Sentence Level weighted sum 
- Say, sentence is: P(w_1, ..., w_n). 
*** Sentence level weighted-sum 
- At sentence level, this can be calculated by 
  weighted_mean_all_d(  P_d(w_1, .., w_n)  ) 
*** Word level weithed-sum 
- At word level, this can be caluclated by 
- product 
  { ... 
    weighted_mean_all_d( P(w_n | w_{n-1},w_{n-2}, w_{n-3} ), 
    weighted_mean_all_d( P(w_n+1 | w_n, w_{n-1}, w_{n-2} ), 
    ... 
    weighted_mean_all_d( P(</s> | ...) ) 
  }
*** Not compatible
- The problem is that, two values are different. Weighted mean on
  sentence level (up to each sentence, prob calculated by each
  document model) produces one value. Product of word level
  probabilities that gained by per word weighted mean produces another
  value. They are generally not that far, but not the same. 

*** Which one should we use? 
- If we want to use "per-word predictability" power, we need to do
  things on word level. Maybe this is more powerful. (and a bit
  slower) 
- If we are not interested in word level, and since our assumption
  simply assumes the underlying document-model generates a
  probablility for each given sentence... Then sentence level is good
  enough.
- Try both? Hmm. 

*** DONE For now?
- Try both?: no. on sentence level.  
- Sentence level. Following strictly to P_d(sentence). 
- Basic premise: A sentence, a probability. Each document model is
  independent (although weakly linked by coll-model, but this is
  not relevant here) 
- Word-level might be useful/needed for "dynamic/better LM". 






