* Preparation
** TODO [#B] Install SRILM on NFS home/bin 

* List of Things to Fix 
** TODO [#A] bug splitta outputs the last "." concatted to the last Word.    
** TODO [#C] feature catall.pl "do not print a file size less than X" 

* Status Memo 
** Current corpus: 
- AFP 2010, as "small" test. 

==== 

* Things to Do 
** Collection Model 

*** DONE (run) Get "target" news files (target corpus) all in one folder 
*** DONE (run) catall and generate collection LM model 
*** [#C] (If subdir needed) TODO? (write script) recursively catall and generate collection model 

** Document Model 
*** DONE (write script) For each file, make each LM model

** Produce single sentence prob. (t) 
*** DONE (write matlab script) weighted-sum 
- input: weight (doc prob), sentence prob, of each document 
- output: weighted average. 

*** (write scripts) P(t) prob 
**** DONE (write debug3 reader) read log_prob
**** TODO (write octave caller) weighted sum 
(Octave side) 
- do the weighted-sum of the values, with uniform weight 
- (need): weighted-sum input format (simple matrix)
- (already have): weighted-sum matlab code 
**** TODO (write octave caller) lambda sum (interpolate) 
**** TODO calc P_coll 
- check collection model file 
- get P_coll (t) (with -debug 3)
**** TODO each P_doc(t) 
- get for each pure P_d(t) (with -debug 3), on all doc 
- calculate lamda*P_d + (1-lamda)*P_coll for each by call octave
**** TODO calc P_(t) by weighted sum 
(Perl side?) 
- store the result for each P_d(t). 
- store the one reulst P(t). 

** Produce conditional prob. 
*** TODO (write scripts) P(h | t) prob 
(perl side) 
- get P(h), using previous section 
- get P_d(t) for each doc. 
(Octave side) 
- do the weighted-sum of the P_d(h) values, with (P_d(t) /
  sigma(P_d(t))) as weight for each d. 

*** TODO write script "evidence calculation code" 

===
===
===

* Some Questions 

** Discount related questions
- "Warning: count of count x is zero -- lowering maxcount" 
- "Warning: discount coeff n is out of range: 0" 
It seems that both related to sparseness. Not critical, but affecting
(e.g. less good smoothing?)  

*** TODO Know what are the basic smoothing method, in DEFAULT (no opt) 

* call parameters 


* Additional notes 
- "-bayes 0" mix-model is generally what I would expect from simple
  summation: simple (lambda * model 1 prob) + ((1-lamba) * model 2
  prob), for each word point. (Well if you ask me what -bayes non-zero
  means ... I don't) 
- (like parameters) 


