* EXPERIMENTS 
** Data Preparation
*** TODO [#A] Medium & Large size "collection" model and its preparation  
- currently: 1) afp 2010 (no problem) 
- then: 2) all afp. 
- and: 3) all of the gigaword? 
- Maybe we need something between 2), 3). Summing up two or three
  models, etc. ... 
- Single directory (not [agency]-[year]) output is set in gzset_runner.pl. 
- Doing 2). a) run gzset runner, b) cat all, c) call ngram-count
  (consider -write-binary-lm) 
** Some additional ideas 
*** some rough ideas & observations 
- better baseline would be P(h|h), instead of P(h)? (topical
  relatedness gets some even before starting). 
- "gain" (P(h|t) / P(h)) seems to (generally) increases with the
  length of (t & h)  

** Experiments Memo 
*** Currently used/tested SRILM call parameters 
**** ngram-count 
- (CURRENT) "-text" and "-lm", and "-write-binary-lm", all other
  default 
**** ngram 
- (CURRENT) all default: no other than "-ppl" (input designation) and "-lm".  

*** Current corpus 
- P_doc AFP 2009_05, as the "small" set of document models. 
- P_coll AFP 2010, as the "small" collection model. 
- (Moore) AFP 1994 & 1995 as another "small" collection & document 

*** Memo on efficiency
**** Testing on May 2009 AFP news (20k documents) 
- Running P_t sequentially currently takes about 3 min (2:48) on Moore.  
- Multi threads (6) on Gillespie, 58 seconds 


* DEVEL
** Ongoing Improvements 
*** SubDir plays (needed before doing more than AFP one year) 
**** TODO P_t should traverse all subdirs. 
- P_t argument change (and all consequent callers) 
- P_t code change (to traverse and run) 
**** TODO Test of P_h_t_multithread with sketch 
*** Collection model generate from subdirs 
**** DONE For collection - catall to dump everything in subdirs.  

*** Per doc model generate with subdirs 
**** TODO For per-doc models - perstroy_runnner with subdir. 

*** Calculating P_doc with subdirs 
**** TODO P_t should work with sub dirs. (so is P_t_multithread) 
- argument for doc models need to be changed. (for now, its glob (path/*.story) ) 

*** DONE [#B] Install SRILM on NFS home/bin 

** Main line coding 
*** Collection Model 
**** DONE (run) Get "target" news files (target corpus) all in one folder 
**** DONE (run) catall and generate collection LM model 
**** DONE [#C] (If subdir needed) TODO? (write script) recursively catall and generate collection model 
*** Document Model 
**** DONE (write script) For each file, make each LM model
*** Produce single sentence prob. (t) 
**** DONE (write matlab script) weighted-sum 
- input: weight (doc prob), sentence prob, of each document 
- output: weighted average. 
**** (write scripts) P(t) prob 
***** DONE (write debug3 reader) read_log_prob, read_prob
***** DONE (write octave caller) lambda sum (interpolate) 
***** DONE check code for get seq_prob to lambda sum 
***** DONE (srilm caller) write ngram runner
- model 
- options  
- sentence (input) 
***** DONE (write octave caller) weighted sum 
- (need): weighted-sum input format (simple matrix)?
- (already have): weighted-sum matlab code 
***** DONE (write octave caller wrapper) logprob mean 
- use weighted sum with same weights. :-) 
***** DONE calc P_coll 
- check collection model file 
- get P_coll (t) (with -debug 3)
***** DONE each P_doc(t) 
- get for each pure P_d(t) (with -debug 3), on all doc 
- calculate lamda*P_d + (1-lamda)*P_coll for each by call octave
***** DONE calc P_(t) by weighted sum 
- do the weighted-sum of the values, with uniform weight 

*** Produce conditional prob. 
**** DONE (write scripts) P(h | t) prob 
**** DONE write script "evidence calculation code" 
Wow. Finally. 
**** DONE sanity check, more with sketch. 
- on AFP 2009 May

**** TODO [#A] Some possible "look-into" data saving. 
- Starting from P_t, P_h, P_h

*** Experiments with this prototype conditional prob. 
**** TODO [#A] Some test on small corpus
- AFP_ENG 2009 05 
**** TODO [#A] Some test on not-so-small-but-still-small corpus 
- AFP ENG 2010 
**** TODO [#A] Some test on mid-size corpus 
- All AFP
- This is not doable until better "recursive-sub-dir-visit" for; 
- Model making: (catall, and etc for collection model) 


* NOTES & PREVIOUS MEMO 
** Past Improvements 
*** DONE Binary language model 
*** add binary option as default option 
**** DONE collection model description (user's own calling) 
**** DONE perstory_runner.pl (per document model) 
- I believe that ngram automatically loads binary model, so no
  additional coding on model users.  

*** DONE [#A] bug splitta outputs the last "." concatted to the last Word.    
*** TODO? [#C] [??] feature catall.pl "do not print a file size less than X" 
*** TODO? [#C] [Very hard - Possible?] Matrix-ize weighted_sum Octave code. 
*** DONE [#A] [Efficiency] Lamda sum in Perl space. (No octave call) 
- For each news "story" we call twice; once ngram (can't reduce this),
  once octave. Maybe starting up octave each time is
  expansive. Consider this. 
*** DONE [#A] [Efficiency for response] Not using multiple threads/ngram processes

** Postponed improvements: "Good to have, but not critical"
*** TODO? [#C] [Efficiency for throughput] Unable to call two or more instances. 
- Currently, the file to be passed to ngram -ppl is a fixed name. 
- should be improved to temporary random name, or something like
  getName{sent}?
- *Not really important*, since the code does use multithread for P_t, and a
  single instance can utilize many nubmers of threads. 
*** TODO? [#C] If log-sum is only needed as "weighted sum" (use not-tool-small sum)
- we may not need to do the costy log-space-sums. 
- (by multiply weights to a certain degree, so within octave normal range). 
- (using reference_weightedsum, or a improved variation, etc). 
- *Not really important* Only calculated twice, or three times only
  per each P(h|t). Not really critical, compared to other
  efficiency issues. 
- Well, "not needing octave anymore" would be nice but. 

==== 
** Known problems
*** Discount related questions
- When processing document-models; 
- "Warning: count of count x is zero -- lowering maxcount" 
- "Warning: discount coeff n is out of range: 0" 
It seems that both related to sparseness. Not critical, but affecting
(e.g. less good smoothing?)  
** Side notes about tools 
*** SRILM 
**** Interpolate call parameters 
- "-bayes 0" mix-model is generally what I would expect from simple
  summation: simple (lambda * model 1 prob) + ((1-lamba) * model 2
  prob), for each word point. (Well if you ask me what -bayes non-zero
  means ... I don't) 
- so the mixture model call is something like: 
- ngram -lm doc.model -mix-lm collection.model -ppl test.txt -bayes 0 -debug 3 -lambda 0.1

**** Perplexity (per word), as calculated in SRILM 
- ppl = 10^(-logprob / (words - OOVs + sentences))
- ppl1 (without </s>) = 10^(-logprob / (words - OOVs)) 

**** Discount methods in SRILM defult 
- When no option is given, it does Good-Turing discount. (the warnings
  are from those, when counting count of counts, etc) 

*** Octave 
**** Octave "precision" of double is one digit less (than SRILM) 
- Seems like this causes the small amount of difference in the final
  result. (try octave> a = 0.00409898) 
- Octave uses H/W floats. ... hmm. no easy way around(?)
- Eh, no. Above examples is actually within HW float, but octave cuts 
  it. Prolly some precision cut mechanism in work. What's it? 
- "Symbolic toolbox". vpa(something)? Hmm. no need yet.  

** Theoretical crosspoints / decisions 
*** DONE [#A] Word level model, or Sentence level model? 
- Basically, what I am trying to do is doing weighted sum of
  probabilities. There is two way of doing things. 
- Word Level weighted sum and Sentence Level weighted sum 
- Say, sentence is: P(w_1, ..., w_n). 
**** Sentence level weighted-sum 
- At sentence level, this can be calculated by 
  weighted_mean_all_d(  P_d(w_1, .., w_n)  ) 
**** Word level weithed-sum 
- At word level, this can be caluclated by 
- product 
  { ... 
    weighted_mean_all_d( P(w_n | w_{n-1},w_{n-2}, w_{n-3} ), 
    weighted_mean_all_d( P(w_n+1 | w_n, w_{n-1}, w_{n-2} ), 
    ... 
    weighted_mean_all_d( P(</s> | ...) ) 
  }
**** Not compatible
- The problem is that, two values are different. Weighted mean on
  sentence level (up to each sentence, prob calculated by each
  document model) produces one value. Product of word level
  probabilities that gained by per word weighted mean produces another
  value. They are generally not that far, but not the same. 

**** Which one should we use? 
- If we want to use "per-word predictability" power, we need to do
  things on word level. Maybe this is more powerful. (and a bit
  slower) 
- If we are not interested in word level, and since our assumption
  simply assumes the underlying document-model generates a
  probablility for each given sentence... Then sentence level is good
  enough.
- Try both? Hmm. 

**** For now?
- Try both?: no. on sentence level.  
- Sentence level. Following strictly to P_d(sentence). 
- Basic premise: A sentence, a probability. Each document model is
  independent (although weakly linked by coll-model, but this is
  not relevant here) 
- Word-level might be useful/needed for "dynamic/better LM". 






