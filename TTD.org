* Status Memo 
** Current corpus 
- AFP 2010, as "small" test. 

* Preparation
** DONE [#B] Install SRILM on NFS home/bin 

* List of Things to Fix/Improve 
** DONE [#A] bug splitta outputs the last "." concatted to the last Word.    
** TODO? [#C] [??] feature catall.pl "do not print a file size less than X" 
** TODO? [#C] [Very hard - Possible?] Matrix-ize weighted_sum Octave code. 
** TODO [#B] [Efficiency] Lamda sum in Perl space. (No octave call) 
- For each news "story" we call twice; once ngram (can't reduce this),
  once octave. Maybe starting up octave each time is
  expansive. Consider this. 
** TODO [#B] If log-sum is only needed as "weighted sum" (use not-tool-small sum)
- we may not need to do the costy log-space-sums. 
- (by multiply weights to a certain degree, so within octave normal range). 
- (using reference_weightedsum, or a improved variation, etc). 

==== 

* Things to Do 
** Collection Model 

*** DONE (run) Get "target" news files (target corpus) all in one folder 
*** DONE (run) catall and generate collection LM model 
*** [#C] (If subdir needed) TODO? (write script) recursively catall and generate collection model 

** Document Model 
*** DONE (write script) For each file, make each LM model

** Produce single sentence prob. (t) 
*** DONE (write matlab script) weighted-sum 
- input: weight (doc prob), sentence prob, of each document 
- output: weighted average. 

*** (write scripts) P(t) prob 
**** DONE (write debug3 reader) read_log_prob, read_prob
**** DONE (write octave caller) lambda sum (interpolate) 
**** DONE check code for get seq_prob to lambda sum 
**** DONE (srilm caller) write ngram runner
- model 
- options  
- sentence (input) 
**** DONE (write octave caller) weighted sum 
- (need): weighted-sum input format (simple matrix)?
- (already have): weighted-sum matlab code 
**** DONE (write octave caller wrapper) logprob mean 
- use weighted sum with same weights. :-) 
**** DONE calc P_coll 
- check collection model file 
- get P_coll (t) (with -debug 3)
**** TODO each P_doc(t) 
- get for each pure P_d(t) (with -debug 3), on all doc 
- calculate lamda*P_d + (1-lamda)*P_coll for each by call octave
**** TODO calc P_(t) by weighted sum 
- do the weighted-sum of the values, with uniform weight 
- store the result for each P_d(t). 
- store the one reulst P(t). 

** Produce conditional prob. 
*** TODO (write scripts) P(h | t) prob 
(perl side) 
- get P(h), using previous section 
- get P_d(t) for each doc. 
(Octave side) 
- do the weighted-sum of the P_d(h) values, with (P_d(t) /
  sigma(P_d(t))) as weight for each d. 

*** TODO write script "evidence calculation code" 

===
===
===

* Ideas to Consider 


* Possible known problems? 
** Discount related questions
- When processing document-models; 
- "Warning: count of count x is zero -- lowering maxcount" 
- "Warning: discount coeff n is out of range: 0" 
It seems that both related to sparseness. Not critical, but affecting
(e.g. less good smoothing?)  

* Currently used/tested SRILM call parameters 
** ngram-count 
- (CURRENT) all default: no other than "-text" and "-lm". 
** ngram 
- (CURRENT) all default: no other than "-ppl" (input designation) and "-lm".  

* Some notes
** SRILM 
*** Interpolate call parameters 
- "-bayes 0" mix-model is generally what I would expect from simple
  summation: simple (lambda * model 1 prob) + ((1-lamba) * model 2
  prob), for each word point. (Well if you ask me what -bayes non-zero
  means ... I don't) 
- so the mixture model call is something like: 
- ngram -lm doc.model -mix-lm collection.model -ppl test.txt -bayes 0 -debug 3 -lambda 0.1

*** Perplexity (per word), as calculated in SRILM 
- ppl = 10^(-logprob / (words - OOVs + sentences))
- ppl1 (without </s>) = 10^(-logprob / (words - OOVs)) 

*** Discount methods in SRILM defult 
**** TODO Know what are the basic discount/smoothing method, in DEFAULT (no opt)

** Octave 
*** Octave "precision" of double is one digit less (than SRILM) 
- Seems like this causes the small amount of difference in the final
  result. (try octave> a = 0.00409898) 
- Octave uses H/W floats. ... hmm. no easy way around(?)
- "Symbolic toolbox". vpa(something). ... hmm. Maybe. 


* Something I am not really sure

** [#A] Word level model, or Sentence level model? 
- Basically, what I am trying to do is doing weighted sum of
  probabilities. There is two way of doing things. 
- Word Level weighted sum and Sentence Level weighted sum 
- Say, sentence is: P(w_1, ..., w_n). 
*** Sentence level weighted-sum 
- At sentence level, this can be calculated by 
  weighted_mean_all_d(  P_d(w_1, .., w_n)  ) 
*** Word level weithed-sum 
- At word level, this can be caluclated by 
- product 
  { ... 
    weighted_mean_all_d( P(w_n | w_{n-1},w_{n-2}, w_{n-3} ), 
    weighted_mean_all_d( P(w_n+1 | w_n, w_{n-1}, w_{n-2} ), 
    ... 
    weighted_mean_all_d( P(</s> | ...) ) 
  }
*** Not compatible
- The problem is that, two values are different. Weighted mean on
  sentence level (up to each sentence, prob calculated by each
  document model) produces one value. Product of word level
  probabilities that gained by per word weighted mean produces another
  value. They are generally not that far, but not the same. 

*** Which one should we use? 
- If we want to use "per-word predictability" power, we need to do
  things on word level. Maybe this is more powerful. (and a bit
  slower) 
- If we are not interested in word level, and since our assumption
  simply assumes the underlying document-model generates a
  probablility for each given sentence... Then sentence level is good
  enough.
- Try both? Hmm. 

*** For now?
- Try both?: no. on sentence level.  
- Sentence level. Following strictly to P_d(sentence). 
- Basic premise: A sentence, a probability. Each document model is
  independent (although weakly linked by coll-model, but this is
  not relevant here) 

