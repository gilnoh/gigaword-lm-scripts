* some rough ideas & observations 
- better baseline would be P(h|h), instead of P(h)? (topical
  relatedness gets some even before starting). 
- "gain" (P(h|t) / P(h)) seems to (generally) increases with the
  length of (t & h)  

* Status Memo 
** Current corpus 
- P_doc AFP 2009_05, as the "small" set of document models. 
- P_coll AFP 2010, as the "small" collection model. 


* Binary language model 
** add binary option as default option 

* SubDir plays (needed before doing more than AFP one year) 
** Collection model generate from subdirs 
*** DONE For collection - catall to dump everything in subdirs.  

** Per doc model generate with subdirs 
*** TODO For per-doc models - perstroy_runnner with subdir. 

** Calculating P_doc with subdirs 
*** TODO P_t should work with sub dirs. (so is P_t_multithread) 
- argument for doc models need to be changed. (for now, its glob (path/*.story) ) 

* Data Preparation
** TODO [#A] Medium & Large size "collection" model and its preparation  
- currently: 1) afp 2010 (no problem) 
- then: 2) all afp. 
- and: 3) all of the gigaword? 
- Maybe we need something between 2), 3). Summing up two or three
  models, etc. ... 
- Single directory (not [agency]-[year]) output is set in gzset_runner.pl. 
- Doing 2). a) run gzset runner, b) cat all, c) call ngram-count
  (consider -write-binary-lm) 
*** TODO Preparing Medium size collection model 
** DONE [#B] Install SRILM on NFS home/bin 

* List of Things to Fix/Improve 
** TODO [#B] (To reduce load time), binary LM models 
*** TODO On all model building process, use -write-binary-lm 
** DONE [#A] bug splitta outputs the last "." concatted to the last Word.    
** TODO? [#C] [??] feature catall.pl "do not print a file size less than X" 
** TODO? [#C] [Very hard - Possible?] Matrix-ize weighted_sum Octave code. 
** DONE [#A] [Efficiency] Lamda sum in Perl space. (No octave call) 
- For each news "story" we call twice; once ngram (can't reduce this),
  once octave. Maybe starting up octave each time is
  expansive. Consider this. 
** DONE [#A] [Efficiency for response] Not using multiple threads/ngram processes

* Postponed improvements: "Good to have, but not critical"
** TODO? [#C] [Efficiency for throughput] Unable to call two or more instances. 
- Currently, the file to be passed to ngram -ppl is a fixed name. 
- should be improved to temporary random name, or something like
  getName{sent}?
- *Not really important*, since the code does use multithread for P_t, and a
  single instance can utilize many nubmers of threads. 
** TODO? [#C] If log-sum is only needed as "weighted sum" (use not-tool-small sum)
- we may not need to do the costy log-space-sums. 
- (by multiply weights to a certain degree, so within octave normal range). 
- (using reference_weightedsum, or a improved variation, etc). 
- *Not really important* Only calculated twice, or three times only
  per each P(h|t). Not really critical, compared to other
  efficiency issues. 
- Well, "not needing octave anymore" would be nice but. 

==== 

* Main line: Things To Do (codings to do) 
** Collection Model 
*** DONE (run) Get "target" news files (target corpus) all in one folder 
*** DONE (run) catall and generate collection LM model 
*** DONE [#C] (If subdir needed) TODO? (write script) recursively catall and generate collection model 
** Document Model 
*** DONE (write script) For each file, make each LM model
** Produce single sentence prob. (t) 
*** DONE (write matlab script) weighted-sum 
- input: weight (doc prob), sentence prob, of each document 
- output: weighted average. 
*** (write scripts) P(t) prob 
**** DONE (write debug3 reader) read_log_prob, read_prob
**** DONE (write octave caller) lambda sum (interpolate) 
**** DONE check code for get seq_prob to lambda sum 
**** DONE (srilm caller) write ngram runner
- model 
- options  
- sentence (input) 
**** DONE (write octave caller) weighted sum 
- (need): weighted-sum input format (simple matrix)?
- (already have): weighted-sum matlab code 
**** DONE (write octave caller wrapper) logprob mean 
- use weighted sum with same weights. :-) 
**** DONE calc P_coll 
- check collection model file 
- get P_coll (t) (with -debug 3)
**** DONE each P_doc(t) 
- get for each pure P_d(t) (with -debug 3), on all doc 
- calculate lamda*P_d + (1-lamda)*P_coll for each by call octave
**** DONE calc P_(t) by weighted sum 
- do the weighted-sum of the values, with uniform weight 

** Produce conditional prob. 
*** DONE (write scripts) P(h | t) prob 
*** DONE write script "evidence calculation code" 
Wow. Finally. 
*** DONE sanity check, more with sketch. 
- on AFP 2009 May

*** TODO [#A] Some possible "look-into" data saving. 
- Starting from P_t, P_h, P_h

** Experiments with this prototype conditional prob. 
*** TODO [#A] Some test on small corpus
- AFP_ENG 2009 05 
*** TODO [#A] Some test on not-so-small-but-still-small corpus 
- AFP ENG 2010 
*** TODO [#A] Some test on mid-size corpus 
- All AFP
- This is not doable until better "recursive-sub-dir-visit" for; 
- Model making: (catall, and etc for collection model) 


* Efficiency 
** Testing on May 2009 AFP news (20k documents) 
- Running P_t sequentially currently takes about 3 min (2:48) on Moore.  


* Possible known problems? 
** Discount related questions
- When processing document-models; 
- "Warning: count of count x is zero -- lowering maxcount" 
- "Warning: discount coeff n is out of range: 0" 
It seems that both related to sparseness. Not critical, but affecting
(e.g. less good smoothing?)  


* Currently used/tested SRILM call parameters 
** ngram-count 
- (CURRENT) all default: no other than "-text" and "-lm". 
** ngram 
- (CURRENT) all default: no other than "-ppl" (input designation) and "-lm".  


* Some notes
** SRILM 
*** Interpolate call parameters 
- "-bayes 0" mix-model is generally what I would expect from simple
  summation: simple (lambda * model 1 prob) + ((1-lamba) * model 2
  prob), for each word point. (Well if you ask me what -bayes non-zero
  means ... I don't) 
- so the mixture model call is something like: 
- ngram -lm doc.model -mix-lm collection.model -ppl test.txt -bayes 0 -debug 3 -lambda 0.1

*** Perplexity (per word), as calculated in SRILM 
- ppl = 10^(-logprob / (words - OOVs + sentences))
- ppl1 (without </s>) = 10^(-logprob / (words - OOVs)) 

*** Discount methods in SRILM defult 
- When no option is given, it does Good-Turing discount. (the warnings
  are from those, when counting count of counts, etc) 

** Octave 
*** Octave "precision" of double is one digit less (than SRILM) 
- Seems like this causes the small amount of difference in the final
  result. (try octave> a = 0.00409898) 
- Octave uses H/W floats. ... hmm. no easy way around(?)
- Eh, no. Above examples is actually within HW float, but octave cuts 
  it. Prolly some precision cut mechanism in work. What's it? 
- "Symbolic toolbox". vpa(something)? Hmm. no need yet.  


* Theoretical crosspoints / decisions 
** DONE [#A] Word level model, or Sentence level model? 
- Basically, what I am trying to do is doing weighted sum of
  probabilities. There is two way of doing things. 
- Word Level weighted sum and Sentence Level weighted sum 
- Say, sentence is: P(w_1, ..., w_n). 
*** Sentence level weighted-sum 
- At sentence level, this can be calculated by 
  weighted_mean_all_d(  P_d(w_1, .., w_n)  ) 
*** Word level weithed-sum 
- At word level, this can be caluclated by 
- product 
  { ... 
    weighted_mean_all_d( P(w_n | w_{n-1},w_{n-2}, w_{n-3} ), 
    weighted_mean_all_d( P(w_n+1 | w_n, w_{n-1}, w_{n-2} ), 
    ... 
    weighted_mean_all_d( P(</s> | ...) ) 
  }
*** Not compatible
- The problem is that, two values are different. Weighted mean on
  sentence level (up to each sentence, prob calculated by each
  document model) produces one value. Product of word level
  probabilities that gained by per word weighted mean produces another
  value. They are generally not that far, but not the same. 

*** Which one should we use? 
- If we want to use "per-word predictability" power, we need to do
  things on word level. Maybe this is more powerful. (and a bit
  slower) 
- If we are not interested in word level, and since our assumption
  simply assumes the underlying document-model generates a
  probablility for each given sentence... Then sentence level is good
  enough.
- Try both? Hmm. 

*** For now?
- Try both?: no. on sentence level.  
- Sentence level. Following strictly to P_d(sentence). 
- Basic premise: A sentence, a probability. Each document model is
  independent (although weakly linked by coll-model, but this is
  not relevant here) 
- Word-level might be useful/needed for "dynamic/better LM". 
