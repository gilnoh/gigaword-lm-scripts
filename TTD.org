* Status Memo 
** Current corpus 
- P_doc AFP 2009_05, as "small" test. 
- P_coll AFP 2010, as "small" test. 

* Preparation
** TODO [#A] Medium & Large size "collection" model and its preparation  
- currently: 1) afp 2010 (no problem) 
- then: 2) all afp. 
- and: 3) all of the gigaword? 
- Maybe we need something between 2), 3). Summing up two or three
  models, etc. ... 
- Single directory (not [agency]-[year]) output is set in gzset_runner.pl. 
- Doing 2). a) run gzset runner, b) cat all, c) call ngram-count
  (consider -write-binary-lm) 
*** TODO Preparing Medium size collection model 
**** DONE single directory output modification, gzset_runner.pl 
**** DONE run gzset_runner on all AFP_ENG 
**** TODO catall into AFP_ENG_collection.txt 
**** TODO run ngram-count (possibly with binary lm) on collection
**** TODO try to run the sentence, etc. 

** DONE [#B] Install SRILM on NFS home/bin 



* List of Things to Fix/Improve 
** TODO [#B] (To reduce load time), binary LM models 
*** TODO On all model building process, use -write-binary-lm 
** DONE [#A] bug splitta outputs the last "." concatted to the last Word.    
** TODO? [#C] [??] feature catall.pl "do not print a file size less than X" 
** TODO? [#C] [Very hard - Possible?] Matrix-ize weighted_sum Octave code. 
** DONE [#A] [Efficiency] Lamda sum in Perl space. (No octave call) 
- For each news "story" we call twice; once ngram (can't reduce this),
  once octave. Maybe starting up octave each time is
  expansive. Consider this. 
** DONE [#A] [Efficiency for response] Not using multiple threads/ngram processes
** TODO [#C] [Efficiency for throughput] Unable to call two or more instances. 
- Currently, the file to be passed to ngram -ppl is a fixed name. 
- should be improved to temporary random name, or something like
  getName{sent}?
- *Not really important*, since the code does use multithread for P_t, and a
  single instance can utilize many nubmers of threads. 
** TODO [#C] If log-sum is only needed as "weighted sum" (use not-tool-small sum)
- we may not need to do the costy log-space-sums. 
- (by multiply weights to a certain degree, so within octave normal range). 
- (using reference_weightedsum, or a improved variation, etc). 
- *Not really important* Only calculated twice, or three times only. Not
  really critical, compared to other "multi-number" of things. 

==== 


* Things to Do 
** Collection Model 

*** DONE (run) Get "target" news files (target corpus) all in one folder 
*** DONE (run) catall and generate collection LM model 
*** [#C] (If subdir needed) TODO? (write script) recursively catall and generate collection model 

** Document Model 
*** DONE (write script) For each file, make each LM model

** Produce single sentence prob. (t) 
*** DONE (write matlab script) weighted-sum 
- input: weight (doc prob), sentence prob, of each document 
- output: weighted average. 

*** (write scripts) P(t) prob 
**** DONE (write debug3 reader) read_log_prob, read_prob
**** DONE (write octave caller) lambda sum (interpolate) 
**** DONE check code for get seq_prob to lambda sum 
**** DONE (srilm caller) write ngram runner
- model 
- options  
- sentence (input) 
**** DONE (write octave caller) weighted sum 
- (need): weighted-sum input format (simple matrix)?
- (already have): weighted-sum matlab code 
**** DONE (write octave caller wrapper) logprob mean 
- use weighted sum with same weights. :-) 
**** DONE calc P_coll 
- check collection model file 
- get P_coll (t) (with -debug 3)
**** TODO each P_doc(t) 
- get for each pure P_d(t) (with -debug 3), on all doc 
- calculate lamda*P_d + (1-lamda)*P_coll for each by call octave
**** TODO calc P_(t) by weighted sum 
- do the weighted-sum of the values, with uniform weight 
- store the result for each P_d(t). 
- store the one reulst P(t). 

** Produce conditional prob. 
*** TODO (write scripts) P(h | t) prob 
(perl side) 
- get P(h), using previous section 
- get P_d(t) for each doc. 
(Octave side) 
- do the weighted-sum of the P_d(h) values, with (P_d(t) /
  sigma(P_d(t))) as weight for each d. 

*** TODO write script "evidence calculation code" 

===
===
===

* Efficiency 
** Testing on May 2009 AFP news (20k documents) 
- Running P_t sequentially currently takes about 3 min (2:48) on Moore.  
- 

* Possible known problems? 
** Discount related questions
- When processing document-models; 
- "Warning: count of count x is zero -- lowering maxcount" 
- "Warning: discount coeff n is out of range: 0" 
It seems that both related to sparseness. Not critical, but affecting
(e.g. less good smoothing?)  

* Currently used/tested SRILM call parameters 
** ngram-count 
- (CURRENT) all default: no other than "-text" and "-lm". 
** ngram 
- (CURRENT) all default: no other than "-ppl" (input designation) and "-lm".  

* Some notes
** SRILM 
*** Interpolate call parameters 
- "-bayes 0" mix-model is generally what I would expect from simple
  summation: simple (lambda * model 1 prob) + ((1-lamba) * model 2
  prob), for each word point. (Well if you ask me what -bayes non-zero
  means ... I don't) 
- so the mixture model call is something like: 
- ngram -lm doc.model -mix-lm collection.model -ppl test.txt -bayes 0 -debug 3 -lambda 0.1

*** Perplexity (per word), as calculated in SRILM 
- ppl = 10^(-logprob / (words - OOVs + sentences))
- ppl1 (without </s>) = 10^(-logprob / (words - OOVs)) 

*** Discount methods in SRILM defult 
- When no option is given, it does Good-Turing discount. (the warnings
  are from those, when counting count of counts, etc) 

** Octave 
*** Octave "precision" of double is one digit less (than SRILM) 
- Seems like this causes the small amount of difference in the final
  result. (try octave> a = 0.00409898) 
- Octave uses H/W floats. ... hmm. no easy way around(?)
- Eh, no. Above examples is actually within HW float, but octave cuts 
  it. Prolly some precision cut mechanism in work. What's it? 
- "Symbolic toolbox". vpa(something)? Hmm. no need yet.  


* Theoretical crosspoints / decisions 
** DONE [#A] Word level model, or Sentence level model? 
- Basically, what I am trying to do is doing weighted sum of
  probabilities. There is two way of doing things. 
- Word Level weighted sum and Sentence Level weighted sum 
- Say, sentence is: P(w_1, ..., w_n). 
*** Sentence level weighted-sum 
- At sentence level, this can be calculated by 
  weighted_mean_all_d(  P_d(w_1, .., w_n)  ) 
*** Word level weithed-sum 
- At word level, this can be caluclated by 
- product 
  { ... 
    weighted_mean_all_d( P(w_n | w_{n-1},w_{n-2}, w_{n-3} ), 
    weighted_mean_all_d( P(w_n+1 | w_n, w_{n-1}, w_{n-2} ), 
    ... 
    weighted_mean_all_d( P(</s> | ...) ) 
  }
*** Not compatible
- The problem is that, two values are different. Weighted mean on
  sentence level (up to each sentence, prob calculated by each
  document model) produces one value. Product of word level
  probabilities that gained by per word weighted mean produces another
  value. They are generally not that far, but not the same. 

*** Which one should we use? 
- If we want to use "per-word predictability" power, we need to do
  things on word level. Maybe this is more powerful. (and a bit
  slower) 
- If we are not interested in word level, and since our assumption
  simply assumes the underlying document-model generates a
  probablility for each given sentence... Then sentence level is good
  enough.
- Try both? Hmm. 

*** For now?
- Try both?: no. on sentence level.  
- Sentence level. Following strictly to P_d(sentence). 
- Basic premise: A sentence, a probability. Each document model is
  independent (although weakly linked by coll-model, but this is
  not relevant here) 
- Word-level might be useful/needed for "dynamic/better LM". 
