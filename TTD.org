* Status Memo 
** Current corpus 
- AFP 2010, as "small" test. 

* Preparation
** DONE [#B] Install SRILM on NFS home/bin 

* List of Things to Fix/Improve 
** DONE [#A] bug splitta outputs the last "." concatted to the last Word.    
** TODO [#C] [??] feature catall.pl "do not print a file size less than X" 
** TODO [#C] [Possible?] Matrix-ize weighted_sum Octave code. 

==== 

* Things to Do 
** Collection Model 

*** DONE (run) Get "target" news files (target corpus) all in one folder 
*** DONE (run) catall and generate collection LM model 
*** [#C] (If subdir needed) TODO? (write script) recursively catall and generate collection model 

** Document Model 
*** DONE (write script) For each file, make each LM model

** Produce single sentence prob. (t) 
*** DONE (write matlab script) weighted-sum 
- input: weight (doc prob), sentence prob, of each document 
- output: weighted average. 

*** (write scripts) P(t) prob 
**** DONE (write debug3 reader) read_log_prob, read_prob
**** DONE (write octave caller) lambda sum (interpolate) 
**** DONE check code for get seq_prob to lambda sum 
**** DONE (srilm caller) write ngram runner
- model 
- options  
- sentence (input) 
**** DONE (write octave caller) weighted sum 
- (need): weighted-sum input format (simple matrix)?
- (already have): weighted-sum matlab code 
**** DONE (write octave caller wrapper) logprob mean 
- use weighted sum with same weights. :-) 
**** TODO calc P_coll 
- check collection model file 
- get P_coll (t) (with -debug 3)
**** TODO each P_doc(t) 
- get for each pure P_d(t) (with -debug 3), on all doc 
- calculate lamda*P_d + (1-lamda)*P_coll for each by call octave
**** TODO calc P_(t) by weighted sum 
- do the weighted-sum of the values, with uniform weight 
- store the result for each P_d(t). 
- store the one reulst P(t). 

** Produce conditional prob. 
*** TODO (write scripts) P(h | t) prob 
(perl side) 
- get P(h), using previous section 
- get P_d(t) for each doc. 
(Octave side) 
- do the weighted-sum of the P_d(h) values, with (P_d(t) /
  sigma(P_d(t))) as weight for each d. 

*** TODO write script "evidence calculation code" 

===
===
===

* Ideas to Consider 
** If log-sum is only needed as "weighted sum"
- we may not need to do the costy log-space-sums. 
- (by multiply weights to a certain degree, so within octave normal range). 
- (using reference_weightedsum, or a improved variation, etc). 


* Possible known problems? 
** Discount related questions
- When processing document-models; 
- "Warning: count of count x is zero -- lowering maxcount" 
- "Warning: discount coeff n is out of range: 0" 
It seems that both related to sparseness. Not critical, but affecting
(e.g. less good smoothing?)  

* Currently used/tested SRILM call parameters 
** ngram-count 
- (CURRENT) all default: no other than "-text" and "-lm". 
** ngram 
- (CURRENT) all default: no other than "-ppl" (input designation) and "-lm".  

* (Really Minor, prolly not worth) Consider
** Splitta tokenization errors are post-processed for now. 
- But maybe, use it only for sentence split, and not for tokenization,
  and use another tokenization run? 
- Not for now. Just keep an eye, for further problem, if happenes. 

* Some notes
** SRILM 
- "-bayes 0" mix-model is generally what I would expect from simple
  summation: simple (lambda * model 1 prob) + ((1-lamba) * model 2
  prob), for each word point. (Well if you ask me what -bayes non-zero
  means ... I don't) 
** TODO Know what are the basic discount/smoothing method, in DEFAULT (no opt) 
