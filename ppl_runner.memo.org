(devel Memo for PPL runner scripts. permanent memos should go to memo.org ) 

* Stabilize, Eval & Enhancement 
** TODO add more context setters 
*** DONE NULL (useful for half-sentence). 
*** TODO window-1, window-2, window-3 
*** TODO all_else 
*** TODO all_prev 


** TODO enhancement? 
*** TODO </s> equalizer 
*** TODO gz_set_model_maker: remove (or do not make) less than 5 line .story files.  

** TODO some more preliminary testing with something else. (nyt) 
- use all_afp 
- get some random news from nyt. one day? (200+ news?) 

** DONE some preliminary testing with existing test (football) 
*** DONE check first_one concurs with SRILM output itself (surely, but) 
*** DONE prev-2, prev-3  
*** DONE prev 1, with "blending" ON". 
- blending really makes differences... Since P(t|t) is the best. that the model can work on P(t|x). 
- Both P(text | context+text), and P(context+text | context) reported, as expected, far lower perplexity. 
- conditioned on context+text was lower... 
*** DONE add half-sentence-to-context.  
- half-sentence is really conditioning a lot. 
*** DONE prev-one without comma, periods and quotes. 




* Basic Devel 
** DONE clean condprob code
 
** DONE finish multiple sentence PPL checking (lost code) 

** DONE work on ppl_t and ppl_h_given_t 
*** result is now as condprob_h_given_h 
-  [writing on condprob.pm] PPL oriented P(b|a) 
*** PPL_X(text, context) needs to return 
- length of the text: two numbers: count nonOOV words, count sentence. 
- probability: P_col(text), P_model(text), P_model(text | context) 
- Maybe also ppl? but this can be easily get by calc_ppl(logprob, nonOOV, count_sent). 
- well, but maybe: PPL_col (text), PPL_model(text), PPL_conditioned_model(text | context)  

** DONE check new method works Okay 
- compare same sentence with old method. write down output values and check the same. 
- also check the collection value output is the same, with SRILM value. 

** DONE work on PPL runner basic capability 
*** DONE main outline 
*** DONE continue work on ppl_one_doc 
*** DONE expand it to work on multiple files (arguments) 




* PPL run test systemically. 
** PPL runner systemically; design. 
*** TODO how to choose? one session about experiment design. 




* Different way of giving context 
- just previous sentence 
- all previous sentences 
- previous two sentences 
- prev+3, next+3 sentences 
- all other sentences within document (other than this) 








* Misc. Memo 
** issue of commas, periods, and quotes. 
- Ah. interesting. commas, periods and quotes impact a lot. 
- Maybe I should simply get rid of them; both in the training and running. 
- (Maybe at least in running?) 
*** More: the main reason is final . -> </s>. 
- This is the "single-biggest-sure" thing (biggest probability mass)
  in the whole sentence. usually. 
- Document based sum suffers (unfairly, IMO) from this part. (?) 
- The problem is that, for "document" based models, this can be
  fluctuate...(?)
