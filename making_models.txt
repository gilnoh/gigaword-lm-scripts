=== 
0. PREREQUISITE  
- SRILM binaries must be in PATH. (ngram-count and ngram) 
- Perl 5.8 or later needed.  
- Python 2.x needed by the sentence splitter. (Splitta, in /splitta) 

=== 
1. Unpack (the selected) GigaWord target gz files, and run splitta to
do tokenization & sentence splitting. This will form your corpus. 

Commands to run: 
> perl gzset_runner.pl [list of gigaword gz file] 

Note: output folder is defined in the script. For now, let's assume
this is "./models/document/". (actual may differ). This process will
take some time, according to the corpus size. 

===
2. Generate "collection" LM on all of the "story" (actual news
article) files.  

Commands to run: 
1) > perl cat_all_story.pl ./models/document > ./models/collection/collection.txt
2) > ngram-count -text ./models/collection/collection.txt -lm ./models/collection/collection.model -write-binary-lm 

Note: 1) generates single big collection text of news articles,for
      SRILM ngram modeller. 2) runs SRILM ngram model on the big text,
      and saves the resulting model on output/collection/collection.model
Note: If you have use some other options (like order n) for LM, 
      those should also be reflected on the next step. 

=== 
** TODO: make it "look this and subdir" version for perstory_runner.pl 
(just like cat_all_story.pl) 

3. Generate "per document" LM on each of the "story". 

Commands to run: 
> perl perstory_runner.pl "./output/newsarticles/*.story" 

Note: The model files (*.model) will be generated in the newsarticle
dir, with the same name to each .story document file names. 
Note: This also takes some time, according to the corpus size. 

===
4. Now the model files are ready. Collection-wide model on
/output/collection.model, and per-document models are at
/output/newsarticles/[newsname].model 

 See calc_cond_prob.txt for how to use the scripts to use them to
 calculate conditional probability.  
