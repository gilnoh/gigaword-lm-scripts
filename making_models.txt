=== 
0. PREREQUISITE  
- SRILM binaries must be in PATH. (ngram-count and ngram) 
- Perl 5.8 or later needed.  
- Python 2.x needed by the sentence splitter. (Splitta, in /splitta) 
- Java 1.6 or later (required for SOLR search engine) 

=== 
1. Unpack (the selected) GigaWord target gz files, and run splitta to
do tokenization & sentence splitting. This will form your corpus. 

Commands to run: 
> perl gzset_runner.pl [list of gigaword gz file] 

Note: output folder is defined in the script. For now, let's assume
this is "./models/document/". (actual may differ). This process will
take some time, according to the corpus size. 

===
2. Generate "collection" LM on all of the "story" (actual news
article) files.  

Commands to run: 
1) > perl cat_all_story.pl ./models/document > ./models/collection/collection.txt
2) > ngram-count -text ./models/collection/collection.txt -lm ./models/collection/collection.model -write-binary-lm 

Note: 1) generates single big collection text of news articles,for
      SRILM ngram modeller. 2) runs SRILM ngram model on the big text,
      and saves the resulting model on output/collection/collection.model
Note: If you have use some other options (like order n) for LM, 
      those should also be reflected on the next step. 

=== 

3. Generate "per document" LM on each of the "story". 

Commands to run: 
> perl perstory_runner.pl ./models/document 

Note: The model files (*.model) will be generated in the same
directory where the story (news article, *.story) file is located,
with the same name (*.story.model). (as binary SRILM ngram model) 
Note: This also takes some time, according to the corpus size. 

===
4. Now the model files are ready. Collection-wide model on
/models/collection/collection.model, and per-document models are at
/models/document/[newsfile].model. Also, newsfile texts are also there
in /document/ directories. 

Time to index them for faster calculation. 

First of all, activate Apache SOLR search engine. This can be done by
running SOLR. SOLR copy and a specific configuration of that for this
task is included in the code-distribution. 

Command to run: 
> cd solr-4.3.0/gigaword_indexing
(change directory into /solr-4.3.0/gigaword_indexing/) 

> java -jar start.jar 
(this command starts SOLR engine with the given configuration.) 
(Note that you can start SOLR as background with "&", but for now, I
recommend you to keep it running on your command window, just to check
SOLR logs) 

If you see this in the SOLR output, SOLR is now up and running on port
9911: "[main] INFO  org.eclipse.jetty.server.AbstractConnector  â€“
Started SocketConnector@0.0.0.0:9911" 

Now SOLR is up and running. Time to run indexing. Open up another
terminal, go to top directory of the code and run the following
command: 
> perl indexing_solr.pl "./models/document" 
(the command checks each document and index each with SOLR.) 

--- 

Once it is all done, you can look into indexed (gigaword) documents by
visiting SOLR web interface. 
1) Open your browser to: http://localhost:9911/solr/#/
2) On the SOLR interface, click left tab "Core Admin" 
3) Check number of documents indexed, under "Index", "numDocs". 

=== 


# Command to run: 
# > perl indexing.pl ./models/document

# ** TODO: update the text for indexing_SOLR.pl ** 
# Note: this will index all news documents (*.story) within the given
# dir and its direct sub dirs, with Plucene (lucene perl implementation) 
# Note: the resulting index will be stored in ./models_index. (This dir 
# can be changed within the script, but in that case, you should also
# change the global DOCUMENT_INDEX_DIR in proto_condprob.pm. 

# NOTE: If you want to keep multiple indexes --- rename the default
# models_index into something else, and pass this path to the index
# using methods. In that case, you have to pass the path (full arguments
# for P_t and P_t_h index methods, and no longer rely on default index
# paths). 

===
5. Done. 

See next section (calc_cond_prob.txt?) for how to call the scripts 
that uses the models. 

