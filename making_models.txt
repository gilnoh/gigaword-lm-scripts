=== 
0. PREREQUISITE  
- SRILM binaries must be in PATH. (ngram-count and ngram) 
- Perl 5.8 or later needed.  
- Python 2.x needed by the sentence splitter. (Splitta, in /splitta) 
- Java 1.6 or later (required for SOLR search engine) 

0.b Installation 
- clone git 
- unpack solr-4.3.0 binary tarball in the following path. 
  /gigaword-lm-scripts/solr-4.3.0 
 (when freshly cloned, the path only has configurations, no solr binaries)
 (unpack can be done simply by running the following command in the 
  /gigaword-lm-script. 
  > tar xvzf /path-to-downloaded-file/solr-4.3.0.tgz 
  the command will unpack files in the expected 
  /gigaword-lm-script/solr-4.3.0
 )
=== 
1. Unpack (the selected) GigaWord target gz files, and run splitta to
do tokenization & sentence splitting. This will form your corpus. 

Commands to run: 
> perl gzset_runner.pl [list of gigaword gz file] 

Note: output folder is defined in the script. For now, let's assume
this is "./models/document/". (actual may differ). This process will
take some time, according to the corpus size. 

===
2. Generate "collection" LM on all of the "story" (actual news
article) files.  

Commands to run: 
1) > perl cat_all_story.pl ./models/document > ./models/collection/collection.txt
2) > ngram-count -text ./models/collection/collection.txt -lm ./models/collection/collection.model -write-binary-lm 

Note: 1) generates single big collection text of news articles,for
      SRILM ngram modeller. 2) runs SRILM ngram model on the big text,
      and saves the resulting model on output/collection/collection.model
Note: If you have use some other options (like order n) for LM, 
      those should also be reflected on the next step. 

=== 

3. Generate "per document" LM on each of the "story". 

Commands to run: 
> perl perstory_runner.pl ./models/document 

Note: The model files (*.model) will be generated in the same
directory where the story (news article, *.story) file is located,
with the same name (*.story.model). (as binary SRILM ngram model) 
Note: This also takes SOME time, according to the corpus size. 

===
4. Now the model files are ready. Collection-wide model on
/models/collection/collection.model, and per-document models are at
/models/document/[newsfile].model. Also, newsfile texts are also there
in /document/ directories. 

Time to index them for faster calculation. 

First of all, activate Apache SOLR search engine. This can be done by
running SOLR. SOLR copy and a specific configuration of that for this
task is included in the code-distribution. 

Command to run: 
> cd solr-4.3.0/gigaword_indexing
(change directory into /solr-4.3.0/gigaword_indexing/) 

> java -jar start.jar 
(this command starts SOLR engine with the given configuration.) 
(Note that you can start SOLR as background with "&", but for now, I
recommend you to keep it running on your command window, just to check
SOLR logs) 

If you see this in the SOLR output, SOLR is now up and running on port
9911: "[main] INFO  org.eclipse.jetty.server.AbstractConnector  â€“
Started SocketConnector@0.0.0.0:9911" 

Now SOLR is up and running. Time to run indexing. Open up another
terminal, go to top directory of the code and run the following
command: 
> perl indexing_solr.pl "./models/document" 
(the command checks each document and index each with SOLR.) 

This will take some time. (but much faster than per-doc model
generation.) 

--- 

Once it is all done, you can look into indexed (gigaword) documents by
visiting SOLR web interface. 
1) Open your browser to: http://localhost:9911/solr/#/
2) On the SOLR interface, click left tab "Core Admin" 
3) Check number of documents indexed, under "Index", "numDocs". 

===
5. All model preparation is done. Now run a test. 

> perl sketch.pl 

This short code gives you minimal running example of calling codes to
calculate P(text1 | text2). 


